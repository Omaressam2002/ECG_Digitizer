{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8KEbZW3KeBG",
    "outputId": "15649cfd-9ddd-41f2-db00-2ea6d260f0b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ptbsc6yx4nag"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0HjGO1VQq5n",
    "outputId": "c294584c-8e90-4ec4-e6b2-0e076832f1b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input folder: /content/drive/MyDrive/Dataset/ECG/\n",
      "Output folder: /content/drive/MyDrive/Dataset/ECG/augmented\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "DATASET_DIR = \"/content/drive/MyDrive/Dataset/ECG/\"  # change this to your folder\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/Dataset/ECG/augmented\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Input folder:\", DATASET_DIR)\n",
    "print(\"Output folder:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_d2UjBgRBl5",
    "outputId": "c3f96d70-2b3e-4838-85f4-0c8b32b1fa21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation completed!\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(DATASET_DIR)\n",
    "\n",
    "angles = [5, -5, 10, -10, 15, -15, 20, -20, 25, -25, 30, -30, 45, -45, 60, -60,\n",
    "          90, -90, 120, -120, 150, -150, 180]\n",
    "\n",
    "\n",
    "# Higher weight for smaller angles\n",
    "weights = [\n",
    "    9, 9,       # Â±5\n",
    "    9, 9,       # Â±10\n",
    "    9, 9,       # Â±15\n",
    "    9, 9,       # Â±20\n",
    "    9, 9,       # Â±25\n",
    "    5, 5,       # Â±30\n",
    "    5, 5,       # Â±45\n",
    "    5, 5,       # Â±60\n",
    "    3, 3,       # Â±90\n",
    "    3, 3,       # Â±120\n",
    "    1, 1,       # Â±150\n",
    "    1           # 180\n",
    "]\n",
    "\n",
    "\n",
    "def rotate_image(img, angle):\n",
    "    h, w = img.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "    # Compute rotated bounding box to avoid cropping\n",
    "    cos = np.abs(mat[0, 0])\n",
    "    sin = np.abs(mat[0, 1])\n",
    "    new_w = int((h * sin) + (w * cos))\n",
    "    new_h = int((h * cos) + (w * sin))\n",
    "\n",
    "    # Adjust for translation\n",
    "    mat[0, 2] += (new_w / 2) - center[0]\n",
    "    mat[1, 2] += (new_h / 2) - center[1]\n",
    "\n",
    "    rotated = cv2.warpAffine(img, mat, (new_w, new_h), flags=cv2.INTER_LINEAR)\n",
    "    return rotated\n",
    "\n",
    "def zoom_image(img, zoom_factor):\n",
    "    \"\"\"Zoom in or zoom out while keeping final size same.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Zoom in (crop + resize back)\n",
    "    if zoom_factor > 1:\n",
    "        new_h = int(h / zoom_factor)\n",
    "        new_w = int(w / zoom_factor)\n",
    "        y1 = (h - new_h) // 2\n",
    "        x1 = (w - new_w) // 2\n",
    "        cropped = img[y1:y1+new_h, x1:x1+new_w]\n",
    "        return cv2.resize(cropped, (w, h))\n",
    "\n",
    "    # Zoom out (pad + resize back)\n",
    "    else:\n",
    "        small = cv2.resize(img, (int(w * zoom_factor), int(h * zoom_factor)))\n",
    "        new_img = np.zeros_like(img)\n",
    "        y_start = (h - small.shape[0]) // 2\n",
    "        x_start = (w - small.shape[1]) // 2\n",
    "        new_img[y_start:y_start+small.shape[0], x_start:x_start+small.shape[1]] = small\n",
    "        return new_img\n",
    "\n",
    "def augment_image(img_path, save_dir):\n",
    "    img = cv2.imread(img_path)\n",
    "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "    # 1) Redundant original copies\n",
    "    for i in range(1, 2):\n",
    "        cv2.imwrite(os.path.join(OUTPUT_DIR,f\"{base_name}_orig{i}.png\"), img)\n",
    "\n",
    "    # 2) Zoom in / out\n",
    "    zoom_in_factor = random.uniform(1,1.25)\n",
    "    zoom_out_factor = random.uniform(0.75,1)\n",
    "    zoom_in = zoom_image(img, zoom_in_factor)\n",
    "    zoom_out = zoom_image(img, zoom_out_factor)\n",
    "\n",
    "    cv2.imwrite(os.path.join(OUTPUT_DIR,f\"{base_name}_zoom_in.png\"), zoom_in)\n",
    "    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_zoom_out.png\"), zoom_out)\n",
    "\n",
    "    # 3) 3 rotations w/ weighted probability\n",
    "    for i in range(1, 3):\n",
    "        angle = random.choices(angles, weights=weights, k=1)[0]\n",
    "        rot_angle = angle + random.uniform(-1.5,1.5)\n",
    "        rotated = rotate_image(img, rot_angle)\n",
    "        cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_rot_{angle}_{i}_soft.png\"), rotated)\n",
    "    for i in range(1, 5):\n",
    "        angle = random.choices(angles, weights=weights, k=1)[0]\n",
    "        rot_angle = angle\n",
    "        rotated = rotate_image(img, rot_angle)\n",
    "        cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_rot_{angle}_{i}_hard.png\"), rotated)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "files = os.listdir(DATASET_DIR)\n",
    "for image_path in files:\n",
    "    if not image_path.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        continue\n",
    "    augment_image(os.path.join(DATASET_DIR,image_path), OUTPUT_DIR)\n",
    "\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "# import os\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     image_paths = [os.path.join(DATASET_DIR, f) for f in os.listdir(DATASET_DIR)]\n",
    "\n",
    "#     with Pool(cpu_count()) as p:\n",
    "#         p.starmap(augment_image, [(img_path, OUTPUT_DIR) for img_path in image_paths])\n",
    "\n",
    "#     print(\"Augmentation completed!\")\n",
    "\n",
    "print(\"Augmentation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J79QsiaLewaD",
    "outputId": "f43351a2-26d9-4e96-8429-15e2d40ad44a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1065"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEBdoon2cuhz",
    "outputId": "f31584ff-a837-4d09-e490-cc366654b43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 93 files from augmented â†’ input folder!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "count = 0\n",
    "DATASET_DIR = \"/content/drive/MyDrive/Dataset/ECG/\"\n",
    "for filename in os.listdir(DATASET_DIR):\n",
    "    src_path = os.path.join(DATASET_DIR, filename)\n",
    "    dst_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    if os.path.isfile(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        count += 1\n",
    "\n",
    "print(f\"Copied {count} files from augmented â†’ input folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4O5ufb98eaAZ",
    "outputId": "b25c85e8-07a1-4687-bb9e-f7893e3d38c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 1065\n",
      "Moving 106 images to validation folder...\n",
      "Done!\n",
      "Validation images moved: 106\n",
      "Training images remaining: 959\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "DATASET_DIR = \"/content/drive/MyDrive/Dataset/ECG/augmented/\"\n",
    "VAL_DIR = \"/content/drive/MyDrive/Dataset/ECG_val/\"\n",
    "\n",
    "# Create validation directory if not exists\n",
    "os.makedirs(VAL_DIR, exist_ok=True)\n",
    "\n",
    "# List all image files\n",
    "all_files = [\n",
    "    f for f in os.listdir(DATASET_DIR)\n",
    "    if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "]\n",
    "\n",
    "print(f\"Total images found: {len(all_files)}\")\n",
    "\n",
    "# Shuffle deterministically for reproducibility\n",
    "random.seed(42)\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# Compute number of validation samples (10%)\n",
    "val_count = int(0.10 * len(all_files))\n",
    "\n",
    "val_files = all_files[:val_count]  # first 10% after shuffle\n",
    "\n",
    "print(f\"Moving {val_count} images to validation folder...\")\n",
    "\n",
    "moved = 0\n",
    "for filename in val_files:\n",
    "    src_path = os.path.join(DATASET_DIR, filename)\n",
    "    dst_path = os.path.join(VAL_DIR, filename)\n",
    "\n",
    "    if os.path.isfile(src_path):\n",
    "        shutil.move(src_path, dst_path)\n",
    "        moved += 1\n",
    "\n",
    "print(\"Done!\")\n",
    "print(f\"Validation images moved: {moved}\")\n",
    "print(f\"Training images remaining: {len(all_files) - moved}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Hn0rqoAg48hk"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/local_data/\n",
    "!rm -rf /content/local_val/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Ods7eecqFHkD"
   },
   "outputs": [],
   "source": [
    "!cp -r \"/content/drive/MyDrive/Dataset/ECG/augmented\" \"/content/local_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yLzjreXLFKiI"
   },
   "outputs": [],
   "source": [
    "!cp -r \"/content/drive/MyDrive/Dataset/ECG_val\" \"/content/local_val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EP-vCbIBXevp",
    "outputId": "b70a23e5-5a8f-474d-8190-896fc73cdade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.83M/9.83M [00:00<00:00, 92.5MB/s]\n",
      "/tmp/ipython-input-1933980987.py:104: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Epoch 1/25:   0%|          | 0/11 [00:00<?, ?it/s]/tmp/ipython-input-1933980987.py:130: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch 1/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:23<00:00, 23.94s/it, loss=0.992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.4422\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:29<00:00, 24.47s/it, loss=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.5737\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:29<00:00, 24.54s/it, loss=0.0497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.6215\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:26<00:00, 24.19s/it, loss=0.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.7052\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:44<00:00, 25.90s/it, loss=0.496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.8725\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:29<00:00, 24.47s/it, loss=0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.8884\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:33<00:00, 24.89s/it, loss=0.0332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9084\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:21<00:00, 23.74s/it, loss=0.0847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:25<00:00, 24.11s/it, loss=0.0561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9402\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:33<00:00, 24.83s/it, loss=0.0161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9641\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:21<00:00, 23.79s/it, loss=0.0155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9721\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:34<00:00, 24.96s/it, loss=0.00178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:25<00:00, 24.11s/it, loss=0.0647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:46<00:00, 26.06s/it, loss=0.0346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9761\n",
      "ðŸ”¥ Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:24<00:00, 24.02s/it, loss=0.0212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:27<00:00, 24.36s/it, loss=0.00593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:31<00:00, 24.68s/it, loss=0.00119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:29<00:00, 24.48s/it, loss=0.00298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6/11 [02:48<02:20, 28.10s/it, loss=0.00248]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1933980987.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1933980987.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_angle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_DIR = \"/content/local_data/\"\n",
    "VAL_DIR = \"/content/local_val/\"\n",
    "\n",
    "ANGLE_CLASSES = [0, 5, -5, 10, -10, 15, -15, 20, -20, 25, -25, 30, -30, 45, -45, 60, -60,\n",
    "                90, -90, 120, -120, 150, -150, 180]\n",
    "\n",
    "angle_to_idx = {a: i for i, a in enumerate(ANGLE_CLASSES)}\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = folder\n",
    "        self.files = [f for f in os.listdir(folder) if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def extract_angle(self, filename):\n",
    "        \"\"\"\n",
    "        Extract rotation angle from filename.\n",
    "        Valid cases:\n",
    "          - *_rot_-45_3.png â†’ angle = -45\n",
    "          - *_rot_30.png    â†’ angle = 30\n",
    "          - *_orig2.png     â†’ angle = 0\n",
    "          - *_zoom_in.png   â†’ angle = 0\n",
    "          - *_zoom_out.png  â†’ angle = 0\n",
    "          - *.png (no tag)  â†’ angle = 0\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Check for rotation pattern: \"_rot_<angle>\"\n",
    "        rot_match = re.search(r\"_rot_(-?\\d+)\", filename)\n",
    "        if rot_match:\n",
    "            angle = int(rot_match.group(1))\n",
    "            return angle_to_idx[angle]\n",
    "\n",
    "        # 2) Any augmentation that does not specify angle â†’ assume 0 (label index for zero)\n",
    "        zero_angle = 0\n",
    "        return angle_to_idx[zero_angle]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        path = os.path.join(self.folder, fname)\n",
    "\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        y = self.extract_angle(fname)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, y\n",
    "\n",
    "# imageNet normalization\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.1,0.1,0.1,0.05)\n",
    "    ], p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_ds = AngleDataset(DATASET_DIR, transform=train_tf)\n",
    "val_ds   = AngleDataset(VAL_DIR, transform=val_tf)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "model = models.mobilenet_v3_small(weights=\"IMAGENET1K_V1\")\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, len(ANGLE_CLASSES))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x)\n",
    "            correct += (preds.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loop = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Auto Mixed Precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    acc = evaluate()\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Val Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), f\"best_mobilenetv3_angle_{acc}.pth\")\n",
    "        print(\"ðŸ”¥ Saved new best model!\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7OFoxnR-fZRW",
    "outputId": "1b9de82e-4ee1-4a99-c3c9-ceeb91c22d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input folder: /content/drive/MyDrive/Dataset/ECG/\n",
      "Output folder: /content/drive/MyDrive/Dataset/ECG/augmented\n",
      "Augmentation completed!\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(DATASET_DIR)\n",
    "\n",
    "angles = [5, -5, 10, -10, 15, -15, 20, -20, 25, -25, 30, -30, 45, -45, 60, -60,\n",
    "          90, -90, 120, -120, 150, -150, 180]\n",
    "\n",
    "\n",
    "DATASET_DIR = \"/content/drive/MyDrive/Dataset/ECG/\"  # change this to your folder\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/Dataset/ECG/augmented\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Input folder:\", DATASET_DIR)\n",
    "print(\"Output folder:\", OUTPUT_DIR)\n",
    "\n",
    "# Higher weight for smaller angles\n",
    "weights = [\n",
    "    9, 9,       # Â±5\n",
    "    9, 9,       # Â±10\n",
    "    9, 9,       # Â±15\n",
    "    9, 9,       # Â±20\n",
    "    9, 9,       # Â±25\n",
    "    5, 5,       # Â±30\n",
    "    5, 5,       # Â±45\n",
    "    5, 5,       # Â±60\n",
    "    3, 3,       # Â±90\n",
    "    3, 3,       # Â±120\n",
    "    1, 1,       # Â±150\n",
    "    1           # 180\n",
    "]\n",
    "\n",
    "\n",
    "def rotate_image(img, angle):\n",
    "    h, w = img.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "    # Compute rotated bounding box to avoid cropping\n",
    "    cos = np.abs(mat[0, 0])\n",
    "    sin = np.abs(mat[0, 1])\n",
    "    new_w = int((h * sin) + (w * cos))\n",
    "    new_h = int((h * cos) + (w * sin))\n",
    "\n",
    "    # Adjust for translation\n",
    "    mat[0, 2] += (new_w / 2) - center[0]\n",
    "    mat[1, 2] += (new_h / 2) - center[1]\n",
    "\n",
    "    rotated = cv2.warpAffine(img, mat, (new_w, new_h), flags=cv2.INTER_LINEAR)\n",
    "    return rotated\n",
    "\n",
    "\n",
    "def augment_image(img_path, save_dir):\n",
    "    img = cv2.imread(img_path)\n",
    "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "\n",
    "    # 3) 3 rotations w/ weighted probability\n",
    "    for i in range(3, 5):\n",
    "        angle = random.choices(angles, weights=weights, k=1)[0]\n",
    "        rot_angle = angle + random.uniform(-1.5,1.5)\n",
    "        rotated = rotate_image(img, rot_angle)\n",
    "        cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_rot_{angle}_{i}_soft.png\"), rotated)\n",
    "    for i in range(3, 5):\n",
    "        angle = random.choices(angles, weights=weights, k=1)[0]\n",
    "        rot_angle = angle\n",
    "        rotated = rotate_image(img, rot_angle)\n",
    "        cv2.imwrite(os.path.join(OUTPUT_DIR, f\"{base_name}_rot_{angle}_{i}_hard.png\"), rotated)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "files = os.listdir(DATASET_DIR)\n",
    "for image_path in files:\n",
    "    if not image_path.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        continue\n",
    "    augment_image(os.path.join(DATASET_DIR,image_path), OUTPUT_DIR)\n",
    "\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "# import os\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     image_paths = [os.path.join(DATASET_DIR, f) for f in os.listdir(DATASET_DIR)]\n",
    "\n",
    "#     with Pool(cpu_count()) as p:\n",
    "#         p.starmap(augment_image, [(img_path, OUTPUT_DIR) for img_path in image_paths])\n",
    "\n",
    "#     print(\"Augmentation completed!\")\n",
    "\n",
    "print(\"Augmentation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkzaM0gO7-Jk"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/content/best_mobilenetv3_angle_0.9760956175298805.pth\"  # example name\n",
    "\n",
    "model = models.mobilenet_v3_small(weights=\"IMAGENET1K_V1\")\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, len(ANGLE_CLASSES))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ðŸ”¥ load weights\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "\n",
    "print(\"Checkpoint loaded! Continuing training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0fjnvbAvykq_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_DIR = \"/content/local_data/\"\n",
    "VAL_DIR = \"/content/local_val/\"\n",
    "\n",
    "ANGLE_CLASSES = [0, 5, -5, 10, -10, 15, -15, 20, -20, 25, -25, 30, -30, 45, -45, 60, -60,\n",
    "                90, -90, 120, -120, 150, -150, 180]\n",
    "\n",
    "angle_to_idx = {a: i for i, a in enumerate(ANGLE_CLASSES)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_ds = AngleDataset(DATASET_DIR, transform=train_tf)\n",
    "val_ds   = AngleDataset(VAL_DIR, transform=val_tf)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QsvgdJb8NHF",
    "outputId": "45710821-12a9-4355-ecc1-eba25071f30a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Continued Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipython-input-490428507.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Continued Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [07:07<00:00, 28.52s/it, loss=0.0798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:47<00:00, 27.15s/it, loss=0.0315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [07:11<00:00, 28.78s/it, loss=0.0516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:27<00:00, 25.83s/it, loss=0.0099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:41<00:00, 26.79s/it, loss=0.00683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9802\n",
      "ðŸ”¥ Saved improved model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:22<00:00, 25.53s/it, loss=0.00425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9831\n",
      "ðŸ”¥ Saved improved model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:39<00:00, 26.66s/it, loss=0.0112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9859\n",
      "ðŸ”¥ Saved improved model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:43<00:00, 26.87s/it, loss=0.0534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:27<00:00, 25.85s/it, loss=0.0456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9915\n",
      "ðŸ”¥ Saved improved model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Continued Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:30<00:00, 26.06s/it, loss=0.00666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.9944\n",
      "ðŸ”¥ Saved improved model!\n"
     ]
    }
   ],
   "source": [
    "EXTRA_EPOCHS = 10\n",
    "\n",
    "for epoch in range(10):   # continue numbering\n",
    "    model.train()\n",
    "    loop = tqdm(train_dl, desc=f\"Continued Epoch {epoch+1}\")\n",
    "\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    acc = evaluate()\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Val Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Save new best\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), f\"best_mobilenetv3_angle_{acc}.pth\")\n",
    "        print(\"ðŸ”¥ Saved improved model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OG2A1xbe8mcO",
    "outputId": "d637e23a-7ee6-4479-cbf1-056f6c316a1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 4\n",
      "0 23\n"
     ]
    }
   ],
   "source": [
    "def get_misclassified_images(model, dataset):\n",
    "    \"\"\"\n",
    "    Returns a list of full file paths for misclassified images in the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    wrong_paths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(dataset)):\n",
    "            img, label = dataset[idx]\n",
    "            path = os.path.join(dataset.folder, dataset.files[idx])\n",
    "\n",
    "            img_input = img.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            pred = model(img_input).argmax(1).item()\n",
    "\n",
    "            if pred != label:\n",
    "                wrong_paths.append(path)\n",
    "                print(pred,label)\n",
    "\n",
    "    return wrong_paths\n",
    "\n",
    "wrong_samples = get_misclassified_images(model, val_ds)\n",
    "print(f\"Total misclassified: {len(wrong_samples)}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for p in wrong_samples:\n",
    "    image = Image.open(p).convert(\"RGB\")\n",
    "    plt.subplot(1, len(wrong_samples), wrong_samples.index(p) + 1)\n",
    "    plt.imshow(image)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeFtTicPQ6GW",
    "outputId": "37bd374d-4214-48fe-a8c3-ef4142eca0d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/local_val/1277179302-0006_rot_-10_3_hard.png',\n",
       " '/content/local_val/121471389-0003_rot_180_4_soft.png']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vexIwculU-sa"
   },
   "outputs": [],
   "source": [
    "ANGLE_CLASSES[6], ANGLE_CLASSES[0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
